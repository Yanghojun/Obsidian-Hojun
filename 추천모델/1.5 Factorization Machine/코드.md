```python
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.feature_extraction import DictVectorizer
from sklearn.preprocessing import StandardScaler
import numpy as np

# ----------------------
# 1. 샘플 데이터 (with label)
# ----------------------
sample_data = [
    {'user_id': 'u1', 'gender': 'male', 'age': '20s', 'region': 'Seoul',
     'activity': 3.2, 'item_id': 'i101', 'category': 'book', 'brand': 'BrandA'},
    {'user_id': 'u2', 'gender': 'female', 'age': '30s', 'region': 'Busan',
     'activity': 5.5, 'item_id': 'i102', 'category': 'electronics', 'brand': 'BrandB'},
    {'user_id': 'u3', 'gender': 'female', 'age': '40s', 'region': 'Daegu',
     'activity': 1.0, 'item_id': 'i103', 'category': 'clothes', 'brand': 'BrandC'},
    {'user_id': 'u1', 'gender': 'male', 'age': '20s', 'region': 'Seoul',
     'activity': 2.0, 'item_id': 'i102', 'category': 'electronics', 'brand': 'BrandB'},
]

# 레이블: 구매 여부 (binary)
labels = [1, 0, 0, 1]  # 예: 1 = 구매함, 0 = 안 함

# ----------------------
# 2. One-hot 인코딩 (DictVectorizer)
# ----------------------
v = DictVectorizer(sparse=False)
X_raw = v.fit_transform(sample_data)

# ----------------------
# 3. 수치형 feature 정규화
# ----------------------
# 수치형 인덱스 찾기
feature_names = v.get_feature_names_out()
numeric_idx = [i for i, name in enumerate(feature_names) if name == "activity"]

# StandardScaler로 정규화
scaler = StandardScaler()
X_raw[:, numeric_idx] = scaler.fit_transform(X_raw[:, numeric_idx])

# ----------------------
# 4. 텐서 변환
# ----------------------
X_tensor = torch.tensor(X_raw, dtype=torch.float32)
y_tensor = torch.tensor(labels, dtype=torch.float32).unsqueeze(1)

# ----------------------
# 5. FM 모델 정의
# ----------------------
class FactorizationMachine(nn.Module):
    def __init__(self, num_features, k):
        super().__init__()
        self.linear = nn.Linear(num_features, 1)
        self.V = nn.Parameter(torch.randn(num_features, k))  # 임베딩 행렬

    def forward(self, x):
        linear_part = self.linear(x)
        interaction_part = 0.5 * torch.sum(
            torch.pow(torch.matmul(x, self.V), 2) - 
            torch.matmul(torch.pow(x, 2), torch.pow(self.V, 2)),
            dim=1, keepdim=True
        )
        return torch.sigmoid(linear_part + interaction_part)  # binary classification

# ----------------------
# 6. 학습 준비
# ----------------------
model = FactorizationMachine(num_features=X_tensor.shape[1], k=10)
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# ----------------------
# 7. 학습 루프
# ----------------------
for epoch in range(100):
    model.train()
    optimizer.zero_grad()
    y_pred = model(X_tensor)
    loss = criterion(y_pred, y_tensor)
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 10 == 0:
        acc = ((y_pred > 0.5) == y_tensor).float().mean().item()
        print(f"Epoch {epoch+1} - Loss: {loss.item():.4f}, Accuracy: {acc:.2f}")

```

# 주목할 점
- 범주형 데이터가 원핫인코딩되고, 수치형 데이터가 정규화(StandardScaler)를 거쳐서 `Input Tensor`가 됌.
- nn.Linear(Input_dim, 1) 을 통해 FM 수식의 $w_0 + \sum^{n}_{i=1}w_ix_i$ 구현
	- $w_i$: 스칼라
	- $x_i$: 스칼라
- 
